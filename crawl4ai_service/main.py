import os
import asyncio
import requests
from fastapi import FastAPI, HTTPException, Request
from fastapi.responses import JSONResponse
from pydantic import BaseModel
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, BrowserConfig, CacheMode
from dotenv import load_dotenv
import logging

# Load environment variables from .env file
load_dotenv()

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = FastAPI()

SERPER_API_KEY = os.getenv("SERPER_API_KEY")

if not SERPER_API_KEY:
    logger.warning("SERPER_API_KEY not found in environment variables. The service might not work.")

# Pydantic model for the request body
class SearchRequest(BaseModel):
    query: str
    limit: int = 5 # Default limit for number of search results to process

# Pydantic model for the source object in the response
class Source(BaseModel):
    url: str
    title: str | None = None
    description: str | None = None
    content: str | None = None # Will be populated with markdown
    markdown: str | None = None
    publishedDate: str | None = None # crawl4ai might not provide this directly
    author: str | None = None      # crawl4ai might not provide this directly
    image: str | None = None
    favicon: str | None = None
    siteName: str | None = None

@app.post("/search", response_model=list[Source])
async def search_and_scrape(search_request: SearchRequest, http_request: Request):
    query = search_request.query
    limit = search_request.limit
    request_id = http_request.headers.get("X-Request-ID", "N/A") # For tracing if provided

    logger.info(f"[{request_id}] Received search request for query: '{query}', limit: {limit}")

    if not SERPER_API_KEY:
        logger.error(f"[{request_id}] Serper API key not configured.")
        raise HTTPException(status_code=500, detail="Search service not configured (missing API key).")

    # Step 1: Fetch search results from Serper API
    search_url = "https://google.serper.dev/search"
    search_payload = {
        "q": query,
        "num": limit * 2 # Fetch more results to have a buffer in case some pages fail to scrape
    }
    search_headers = {
        "X-API-KEY": SERPER_API_KEY,
        "Content-Type": "application/json"
    }

    try:
        logger.info(f"[{request_id}] Calling Serper API for query: {query}")
        response = requests.post(search_url, headers=search_headers, json=search_payload, timeout=10)
        response.raise_for_status() # Raise an exception for HTTP errors
        search_results = response.json()
        logger.info(f"[{request_id}] Serper API responded successfully.")
    except requests.exceptions.RequestException as e:
        logger.error(f"[{request_id}] Error calling Serper API: {e}")
        raise HTTPException(status_code=503, detail=f"Error fetching search results: {e}")

    urls_to_scrape = []
    if "organic" in search_results:
        for item in search_results["organic"][:limit]: # Process up to the requested limit
            if "link" in item:
                urls_to_scrape.append({"url": item["link"], "title": item.get("title"), "description": item.get("snippet")})

    if not urls_to_scrape:
        logger.info(f"[{request_id}] No URLs found from Serper for query: {query}")
        return []

    logger.info(f"[{request_id}] URLs to scrape: {[u['url'] for u in urls_to_scrape]}")

    # Step 2: Scrape URLs with crawl4ai
    scraped_sources: list[Source] = []

    # Configure crawl4ai: attempt to match firecrawl's onlyMainContent
    # by using "fit_markdown" which is generated by default.
    # crawl4ai's default markdown generator uses a PruningContentFilter.
    # We can adjust its parameters if needed, but defaults are often good.
    browser_config = BrowserConfig(headless=True, verbose=False) # Keep verbose=False for cleaner logs unless debugging
    run_config = CrawlerRunConfig(
        cache_mode=CacheMode.ENABLED, # Enable caching for repeated requests to same URL
        # DefaultMarkdownGenerator with PruningContentFilter is used by default
        # We want to ensure we get the main content, "fit_markdown" should help.
    )

    async with AsyncWebCrawler(config=browser_config) as crawler:
        tasks = []
        for url_data in urls_to_scrape:
            logger.info(f"[{request_id}] Creating task to scrape URL: {url_data['url']}")
            # Pass initial title and description from Serper as fallback
            tasks.append(scrape_url(crawler, url_data["url"], run_config, request_id, url_data["title"], url_data["description"]))

        results = await asyncio.gather(*tasks, return_exceptions=True)

    for result_item in results:
        if isinstance(result_item, Source):
            scraped_sources.append(result_item)
        elif isinstance(result_item, Exception):
            # Log the exception, but don't let one failed scrape stop others.
            # The specific error is already logged in scrape_url
            pass

    logger.info(f"[{request_id}] Successfully scraped {len(scraped_sources)} sources out of {len(urls_to_scrape)}.")
    return scraped_sources

async def scrape_url(crawler: AsyncWebCrawler, url: str, config: CrawlerRunConfig, request_id: str, initial_title: str | None, initial_description: str | None) -> Source | None:
    try:
        logger.info(f"[{request_id}] Starting crawl for URL: {url}")
        result = await crawler.arun(url=url, config=config)

        if result and result.status == "completed" and result.markdown:
            markdown_content = result.markdown.fit_markdown if result.markdown.fit_markdown else result.markdown.raw_markdown

            # Extract metadata. crawl4ai's metadata field is a dict.
            # The exact fields might differ from firecrawl's output structure.
            # We need to map them.
            meta = result.metadata if result.metadata else {}

            # Prioritize crawl4ai's metadata, then Serper's, then None
            title = meta.get("title") or initial_title
            description = meta.get("description") or initial_description

            # Image: crawl4ai might have 'og_image' or 'image' in media or metadata
            image = meta.get("og_image") or meta.get("image")
            if not image and result.media and isinstance(result.media.get("images"), list) and result.media["images"]:
                image = result.media["images"][0].get("src") # Take the first image found

            # Favicon:
            favicon = meta.get("icon") or meta.get("shortcut icon") # common favicon rel types
            if not favicon and result.media and isinstance(result.media.get("icons"), list) and result.media["icons"]:
                 favicon = result.media["icons"][0].get("href")


            # Site Name:
            site_name = meta.get("og_site_name") or meta.get("application-name")

            # publishedDate and author are harder to get reliably from general scraping.
            # Firecrawl might have specific extractors for these.
            # For now, we'll leave them as None unless crawl4ai provides them directly (unlikely for all sites).
            # Common metadata names for dates: 'article:published_time', 'datePublished', 'publish_date'
            published_date = meta.get("article:published_time") or meta.get("datePublished") or meta.get("publish_date")
            author = meta.get("author") or meta.get("article:author")

            logger.info(f"[{request_id}] Successfully scraped and processed URL: {url}")
            return Source(
                url=url,
                title=title,
                description=description,
                content=markdown_content, # Alias for markdown
                markdown=markdown_content,
                publishedDate=published_date,
                author=author,
                image=image,
                favicon=favicon,
                siteName=site_name
            )
        else:
            logger.warning(f"[{request_id}] Failed to scrape or no markdown content for URL: {url}. Status: {result.status if result else 'N/A'}")
            return None
    except Exception as e:
        logger.error(f"[{request_id}] Exception during scraping URL {url}: {e}", exc_info=True)
        # Return the exception to be handled by the caller, or None
        # For gather, it's better to let it be caught or return a specific error object
        # For simplicity here, we'll just log and it will be filtered out in the main loop
        return None


@app.on_event("startup")
async def startup_event():
    # This is a good place to run crawl4ai-setup if it's needed and idempotent,
    # or ensure browsers are installed. However, crawl4ai-setup is a CLI command.
    # For Playwright, installation is typically done once.
    # We'll rely on the Dockerfile to handle this.
    logger.info("FastAPI application startup complete.")
    if not SERPER_API_KEY:
         logger.warning("SERPER_API_KEY is not set. /search endpoint will not function correctly.")


@app.get("/health")
async def health_check():
    return {"status": "healthy"}

# For local development:
if __name__ == "__main__":
    import uvicorn
    # It's good practice to also create a .env file for local dev with SERPER_API_KEY
    if not SERPER_API_KEY:
        print("Warning: SERPER_API_KEY environment variable not set. Search functionality will fail.")
        print("Please create a .env file with SERPER_API_KEY='your_key' or set it in your environment.")

    # Check if Playwright browsers are installed, helpful for local dev setup
    try:
        from playwright.sync_api import sync_playwright
        with sync_playwright() as p:
            try:
                browser = p.chromium.launch()
                browser.close()
                logger.info("Chromium browser found by Playwright.")
            except Exception:
                logger.error("Chromium browser not found or launch failed. Please run 'playwright install --with-deps chromium'")
                logger.info("You might also need to run: crawl4ai-setup")

    except ImportError:
        logger.error("Playwright not installed. Please install it via requirements.txt and run 'playwright install --with-deps chromium'")

    uvicorn.run(app, host="0.0.0.0", port=8008)
